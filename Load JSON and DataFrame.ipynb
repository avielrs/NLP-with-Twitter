{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Import Tweet JSON ZIP Files and Create DataFrame\n",
    "### This jupyter notebook is used to load the extracted .josnl.gz files and create a dataframe from this file. \n",
    "\n",
    "The jsonl.gz files are made when running the hydrate.py file from github repo [us-pres-elections-2020](https://github.com/echen102/us-pres-elections-2020). The hydrate.py file dehydrates twitter ids with the package twarc which is associated to the 2020 US Presidential Election. These ids were orginally collected by github user [echen102](https://github.com/echen102). The tweets collected are specifically associated with the 2020 US Presidential Election. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load packages to load json file and unzip gz file\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "# load package to find out how long the execution time of the function \n",
    "import time\n",
    "\n",
    "# load file for identifying global path in order to load multiple json files\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GZIP Package\n",
    "Gzip package is not an efficient package to use. Unzipping the gzip file into a jsonl file first onto the computer before loading the file will load the jsonl a lot quicker into python. However, I do not want to unzip the files onto my computer because this will take up a huge amount of storage space that I do not have. Therefore I am using gzip to unzip the file within python and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for loading one json file into a list\n",
    "\n",
    "def load_json(file):\n",
    "    data = []\n",
    "    with open(file) as f:\n",
    "        for jsonObj in f:\n",
    "            dataDict = json.loads(jsonObj)\n",
    "            data.append(dataDict)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Sample json files\n",
    " The original json file contains about 100,000 tweets. Each file contains tweets for each day of the week. I decide to sample 100 tweets for each file because 1. GZIP is not computationally efficient and would take a very long time to compute 100,000 tweets. As well my laptop does not have enough storage space to hold 100,000 tweets per day for five months. It is also important to work with a smaller dataset before working in the cloud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for opening multiple .jsonl.gz files and appending the json files into a dataframe\n",
    "# \n",
    "def jsonl_to_DF(path):\n",
    "\n",
    "    \n",
    "    startTime = time.time()\n",
    "        \n",
    "    \n",
    "    filenames = glob.glob(path + \"/*.jsonl.gz\")\n",
    "\n",
    "    dfs = []\n",
    "    \n",
    "    # open gzip file, read json file into a datframe named dfs\n",
    "    # sample(n=100) is sampling only 100 lines or 100 tweets in each file. \n",
    "    for filename in filenames:\n",
    "        with gzip.open(filename, 'r') as f:\n",
    "            dfs.append(pd.read_json(filename, lines = True))\n",
    "        \n",
    "    # Concatenate all data into one DataFrame\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # save dataframe as .csv, name of file associated with the folder name: 2020-10, 2020-09, 2020-08, 2020-07, 2020-06\n",
    "    file = path[-7:] + '.csv'\n",
    "    df.to_csv(file)\n",
    "    \n",
    "    #print execuation time of running function\n",
    "    executionTime = (time.time() - startTime) \n",
    "    return 'Execution time in seconds: ' + str(executionTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of path:\n",
    "# path =r'/Users/user_name/Desktop/Tweet_Data/2020-08'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run function to tranform unstructured jsonl.gz files into a structured DataFrame\n",
    "#### DataFrame will be saved as a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_to_DF(r'/Tweet_Data/2020-06')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
